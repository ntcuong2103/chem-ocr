{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check image sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import imagesize\n",
    "shapes = [(fn,imagesize.get('train/'+fn)) for fn in os.listdir('train') if fn.endswith('jpg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of image shapes\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "shapes_np = np.array(list(zip(*shapes))[1])\n",
    "# plt.scatter(shapes_np[:,0],shapes_np[:,1])\n",
    "# plt.xlabel('width')\n",
    "# plt.ylabel('height')\n",
    "\n",
    "# plot with seaborn using scatter plot x and y are the width and height of the images\n",
    "# set xlabel and ylabel to width and height\n",
    "import seaborn as sns\n",
    "sns.jointplot(x=shapes_np[:,0],y=shapes_np[:,1],kind='scatter').set_axis_labels('width','height')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = sorted(shapes,key=lambda x: x[1][0]*x[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fn, _ in (shapes[:10] + shapes[-10:]):\n",
    "    shutil.copy('train/'+fn, 'train_small/'+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes[:10] + shapes[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = [(fn, (w,h), w*h) for fn, (w,h)  in shapes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[s for s in shapes if s[2] > 5e3][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fn, _,_ in [s for s in shapes if s[2] > 5e3][:10]:\n",
    "    shutil.copy('train/'+fn, 'train_small/'+fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([s[2] for s in shapes if s[2] > 5e3])/ len(shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram of sizes\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist([s[2] for s in shapes if s[2] < 4e4], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "shapes = np.array(shapes)\n",
    "shapes.min(axis=0), shapes.max(axis=0), shapes.mean(axis=0), shapes.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes[8933], shapes[10446]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(shapes[:,0] == 7891), np.where(shapes[:,1] == 4686)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check vocab, create vocab map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [line.strip().split('\\t') for line in open('train_ssml_sd.txt').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "vocab_counter = Counter()\n",
    "[vocab_counter.update(line[1].split()) for line in lines]\n",
    "# [line.split() for line in list(zip(*lines))[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = { line.strip().split('\\t')[0]:line.strip().split('\\t')[1]for line in open ('train_ssml_sd_zero.txt').readlines()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict['train_00000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_counter.most_common()[200:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_counter.most_common()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(v,c) for v,c in vocab_counter.items() if c < 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist([c for v,c in vocab_counter.most_common()[100:]], bins=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sort vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_map = {u: v for u, v in [line.strip().split() for line in open('vocab_maps_init.txt').readlines()]}\n",
    "vocab_map.update({v: '<unk>' for v in sorted(vocab_counter.keys())[244:318] if vocab_counter[v] == 1 and v not in vocab_map})\n",
    "vocab_map.update({v: '<unk>' for v in sorted(vocab_counter.keys())[378:] if v not in vocab_map})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab_map.txt', 'w') as f:\n",
    "    for k, v in vocab_map.items():\n",
    "        f.write(f'{k}\\t{v}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(vocab_counter.keys())[244:318]\n",
    "[(v,vocab_counter[v]) for v in sorted(vocab_counter.keys())[244:318] if vocab_counter[v] < 10] # -> <unk>\n",
    "# sorted(vocab_counter.keys())[378:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_map = {u: v for u, v in [line.strip().split('\\t') for line in open('vocab_map.txt').readlines()]}\n",
    "vocab_map.update({v:v for v in vocab_counter.keys() if v not in vocab_map})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab_syms_full.txt', 'w') as f:\n",
    "    f.write('\\n'.join(sorted(set([vocab_map[v] for v in sorted(vocab_counter.keys())]))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab_syms_full.txt', 'w') as f:\n",
    "    f.write('\\n'.join(sorted(vocab_counter.keys())))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vocab import Vocab\n",
    "vocab = Vocab('vocab_syms.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.word2idx[\"<unk>\"], vocab.UNK_IDX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = [line.strip().split('\\t')[1] for line in open('train_ssml_sd.txt').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_tokens = [caption.split() for caption in captions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "token_counter = Counter()\n",
    "\n",
    "token_counter.update(list(map(len, captions_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram of token lengths\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(list(map(len, captions_tokens)), bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check CROHME dataset (offline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "from zipfile import ZipFile\n",
    "from PIL import Image\n",
    "\n",
    "Data = List[Tuple[str, Image.Image, List[str]]]\n",
    "\n",
    "\n",
    "def extract_data(archive: ZipFile, dir_name: str) -> Data:\n",
    "    \"\"\"Extract all data need for a dataset from zip archive\n",
    "\n",
    "    Args:\n",
    "        archive (ZipFile):\n",
    "        dir_name (str): dir name in archive zip (eg: train, test_2014......)\n",
    "\n",
    "    Returns:\n",
    "        Data: list of tuple of image and formula\n",
    "    \"\"\"\n",
    "    with archive.open(f\"{dir_name}/caption.txt\", \"r\") as f:\n",
    "        captions = f.readlines()\n",
    "    data = []\n",
    "    for line in captions:\n",
    "        tmp = line.decode().strip().split()\n",
    "        img_name = tmp[0]\n",
    "        formula = tmp[1:]\n",
    "        with archive.open(f\"{dir_name}/{img_name}.bmp\", \"r\") as f:\n",
    "            # move image to memory immediately, avoid lazy loading, which will lead to None pointer error in loading\n",
    "            img = Image.open(f).copy()\n",
    "        data.append((img_name, img, formula))\n",
    "\n",
    "    print(f\"Extract data from: {dir_name}, with data size: {len(data)}\")\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = extract_data(ZipFile('data.zip'), 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[:5]\n",
    "\n",
    "imgsizes = [img.size for _, img, _ in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgsizes[:5]\n",
    "# plot scatter of image sizes\n",
    "import matplotlib.pyplot as plt\n",
    "# set axis name x: width, y: height\n",
    "plt.scatter(*zip(*imgsizes))\n",
    "plt.xlabel('width')\n",
    "plt.ylabel('height')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nom-ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
